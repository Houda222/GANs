{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda import amp\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# from models import Generator, Discriminator\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "c_  = Fore.GREEN\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhouda-ghallab\u001b[0m (\u001b[33mhouda222\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hghallab/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "API_KEY = \"5156658a1eec69dd17b8fada84a91056f96da322\"\n",
    "\n",
    "wandb.login(key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed          = 42\n",
    "    debug         = False\n",
    "    model_name    = 'DCGAN|FirstGan'\n",
    "    loss_function = 'BCELoss'\n",
    "    train_bs      = 8\n",
    "    valid_bs      = 8\n",
    "    img_size      = [1024, 1024]\n",
    "    # input_size    = [736, 1280]\n",
    "    epochs        = 20\n",
    "    lr            = 2e-4\n",
    "    n_accumulate  = max(1, 32//train_bs)\n",
    "    # num_classes   = 1\n",
    "    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print('> SEEDING DONE')\n",
    "    \n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hghallab/dataset/GANs_dataset/FirstGan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(CFG.img_size),\n",
    "    # torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.io.read_image\n",
    "def load_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = torch.Tensor(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ganData(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=transform, data_dir=data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images_path = data_dir\n",
    "        self.images = os.listdir(self.images_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.images_path))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item_name = self.images[idx]\n",
    "        img = load_img(os.path.join(self.images_path, item_name))\n",
    "        img = self.transform(img)\n",
    "        # img = Image.fromarray(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hghallab/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:449: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "dataset = ganData()\n",
    "\n",
    "if CFG.debug:\n",
    "    train_data, valid_data, _ = torch.utils.data.random_split(dataset, [0.1, 0.1, 0.8])\n",
    "else:\n",
    "    train_data, valid_data = torch.utils.data.random_split(dataset, [1, 0])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=CFG.train_bs, num_workers=0, shuffle=True, pin_memory=True, drop_last=False) # num_workers=0 for debugging else 4\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=CFG.valid_bs, num_workers=0, shuffle=False, pin_memory=True) # num_workers=0 for debugging else 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mvalid_loader\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_item(image, gt=None, pred=None):\n",
    "    image_np = image.cpu().numpy() if image.is_cuda else image.numpy()\n",
    "    image_np = image_np.squeeze()\n",
    "    image_np = np.transpose(image_np, (1, 2, 0)).astype(\"uint8\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_np)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mvalid_loader\u001b[49m)):\n\u001b[1;32m      2\u001b[0m     visualize_item(img)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for img in next(iter(valid_loader)):\n",
    "    visualize_item(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Generator and Discriminator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"data/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n",
    "        self.deconv5_bn = nn.BatchNorm2d(3)\n",
    "        self.deconv6 = nn.ConvTranspose2d(3, 3, 4, 2, 1)\n",
    "        self.deconv6_bn = nn.BatchNorm2d(3)\n",
    "        self.deconv7 = nn.ConvTranspose2d(3, 3, 4, 2, 1)\n",
    "        self.deconv7_bn = nn.BatchNorm2d(3)\n",
    "        self.deconv8 = nn.ConvTranspose2d(3, 3, 4, 2, 1)\n",
    "        self.deconv8_bn = nn.BatchNorm2d(3)\n",
    "        self.deconv9 = nn.ConvTranspose2d(3, 3, 4, 2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.deconv1(input))\n",
    "        # print(\"gen\", input.shape)\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv5_bn(self.deconv5(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv6_bn(self.deconv6(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv7_bn(self.deconv7(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = F.relu(self.deconv8_bn(self.deconv8(x)))\n",
    "        # print(\"gen\", x.shape)\n",
    "        x = torch.tanh(self.deconv9(x))\n",
    "        # print(\"gen\", x.shape)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv6 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv7 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(d*8)        \n",
    "        self.conv8 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(d*8)   \n",
    "        self.conv9 = nn.Conv2d(d*8, 1, 4, 2, 0)\n",
    "\n",
    "        # self.dense = nn.Linear(d*8*32*32, 2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(\"input shape\", input.shape)\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        # print(\"after layer one\", x.shape)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        # print(\"after layer two\", x.shape)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        # print(\"after layer three\", x.shape)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        # print(\"after layer four\", x.shape)\n",
    "        x = F.leaky_relu(self.conv5_bn(self.conv5(x)), 0.2)\n",
    "        # print(\"after layer 5\", x.shape)\n",
    "        x = F.leaky_relu(self.conv6_bn(self.conv6(x)), 0.2)\n",
    "        # print(\"after layer 6\", x.shape)\n",
    "        x = F.leaky_relu(self.conv7_bn(self.conv7(x)), 0.2)\n",
    "        # print(\"after layer 7\", x.shape)\n",
    "        x = F.leaky_relu(self.conv8_bn(self.conv8(x)), 0.2)\n",
    "        # print(\"after layer 8\", x.shape)\n",
    "        # x = self.conv5(x).reshape(-1, 512*59*59)\n",
    "        x = self.conv9(x)\n",
    "        # x = x.reshape(-1, 128*8*32*32)\n",
    "        # x = self.dense(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        # x = torch.sigmoid(self.dense(self.conv5(x).reshape(-1, 512*59*59)))\n",
    "        print(\"output shape, five\", x.shape, \"end\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for generator: 12660967 and discriminator: 78159233\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "device = torch.device(CFG.device)\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "num_params_gen = sum(p.numel() for p in \n",
    "generator.parameters() if p.requires_grad)\n",
    "num_params_disc = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "print('Number of parameters for generator: %d and discriminator: %d' % (num_params_gen, num_params_disc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (deconv1): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (deconv1_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv2): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv4): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv5_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv6): ConvTranspose2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (deconv6_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv7): ConvTranspose2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (dense): Linear(in_features=1048576, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generated_images(generator, latent, epoch):\n",
    "  \"\"\"\n",
    "  Generates and visualizes images from the given generator model.\n",
    "\n",
    "  Args:\n",
    "      generator: The generator model to use for image generation.\n",
    "      num_images: The number of images to generate and visualize.\n",
    "  \"\"\"\n",
    "\n",
    "  # Generate noise vector\n",
    "  noise = torch.randn(CFG.train_bs, latent, device)\n",
    "\n",
    "  # Generate fake images\n",
    "  fake_images = generator(noise)\n",
    "\n",
    "  # Normalize and convert to image format (if necessary)\n",
    "  # fake_images = (fake_images - fake_images.min()) / (fake_images.max() - fake_images.min())  # Normalize\n",
    "  fake_images = fake_images.detach().cpu() * 255  # Convert to 0-255 range (assuming uint8)\n",
    "  fake_images = fake_images.permute(0, 2, 3, 1).to(torch.uint8)  # Permute if channels are not last\n",
    "  \n",
    "  plt.figure(figsize=(24, 3))\n",
    "  for i in range(len(fake_images)):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(fake_images[i])  \n",
    "    plt.axis('off')\n",
    "  plt.suptitle(f\"Generated Images - Epoch {epoch}\", fontsize=12)  # Add title\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WORKSPACE = \"/home/hghallab/Dvlp/GANs/DCGAN\"\n",
    "os.chdir(MODEL_WORKSPACE)\n",
    "runs = sorted(os.listdir(os.path.join(MODEL_WORKSPACE, \"runs\")))\n",
    "if len(runs) == 0:\n",
    "    CURRENT_MODEL_DIR = \"runs/\" + \"run_1\"\n",
    "else:\n",
    "    CURRENT_MODEL_DIR = \"runs/\" + f\"run_{int(runs[-1].split('_')[-1]) + 1}\"\n",
    "os.mkdir(os.path.join(MODEL_WORKSPACE, CURRENT_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDCGAN\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                 config\u001b[38;5;241m=\u001b[39m{k:v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mvars\u001b[39m(CFG))\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k},\n\u001b[1;32m      3\u001b[0m                 name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURRENT_MODEL_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|model-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project='DCGAN', \n",
    "                config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
    "                name=f\"{CURRENT_MODEL_DIR}|model-{CFG.model_name}\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = torch.optim.Adam(params=generator.parameters(), lr=CFG.lr, betas=(0.5, 0.999))\n",
    "disc_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=CFG.lr, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "gen torch.Size([8, 100, 1, 1])\n",
      "gen torch.Size([8, 1024, 4, 4])\n",
      "gen torch.Size([8, 512, 8, 8])\n",
      "gen torch.Size([8, 256, 16, 16])\n",
      "gen torch.Size([8, 128, 32, 32])\n",
      "gen torch.Size([8, 3, 64, 64])\n",
      "gen torch.Size([8, 3, 128, 128])\n",
      "gen torch.Size([8, 3, 256, 256])\n",
      "gen torch.Size([8, 3, 512, 512])\n",
      "gen torch.Size([8, 3, 1024, 1024])\n",
      "info real pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "info fake pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "gen torch.Size([8, 100, 1, 1])\n",
      "gen torch.Size([8, 1024, 4, 4])\n",
      "gen torch.Size([8, 512, 8, 8])\n",
      "gen torch.Size([8, 256, 16, 16])\n",
      "gen torch.Size([8, 128, 32, 32])\n",
      "gen torch.Size([8, 3, 64, 64])\n",
      "gen torch.Size([8, 3, 128, 128])\n",
      "gen torch.Size([8, 3, 256, 256])\n",
      "gen torch.Size([8, 3, 512, 512])\n",
      "gen torch.Size([8, 3, 1024, 1024])\n",
      "info real pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "info fake pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "gen torch.Size([8, 100, 1, 1])\n",
      "gen torch.Size([8, 1024, 4, 4])\n",
      "gen torch.Size([8, 512, 8, 8])\n",
      "gen torch.Size([8, 256, 16, 16])\n",
      "gen torch.Size([8, 128, 32, 32])\n",
      "gen torch.Size([8, 3, 64, 64])\n",
      "gen torch.Size([8, 3, 128, 128])\n",
      "gen torch.Size([8, 3, 256, 256])\n",
      "gen torch.Size([8, 3, 512, 512])\n",
      "gen torch.Size([8, 3, 1024, 1024])\n",
      "info real pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "info fake pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "gen torch.Size([8, 100, 1, 1])\n",
      "gen torch.Size([8, 1024, 4, 4])\n",
      "gen torch.Size([8, 512, 8, 8])\n",
      "gen torch.Size([8, 256, 16, 16])\n",
      "gen torch.Size([8, 128, 32, 32])\n",
      "gen torch.Size([8, 3, 64, 64])\n",
      "gen torch.Size([8, 3, 128, 128])\n",
      "gen torch.Size([8, 3, 256, 256])\n",
      "gen torch.Size([8, 3, 512, 512])\n",
      "gen torch.Size([8, 3, 1024, 1024])\n",
      "info real pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "info fake pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "gen torch.Size([8, 100, 1, 1])\n",
      "gen torch.Size([8, 1024, 4, 4])\n",
      "gen torch.Size([8, 512, 8, 8])\n",
      "gen torch.Size([8, 256, 16, 16])\n",
      "gen torch.Size([8, 128, 32, 32])\n",
      "gen torch.Size([8, 3, 64, 64])\n",
      "gen torch.Size([8, 3, 128, 128])\n",
      "gen torch.Size([8, 3, 256, 256])\n",
      "gen torch.Size([8, 3, 512, 512])\n",
      "gen torch.Size([8, 3, 1024, 1024])\n",
      "info real pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n",
      "info fake pred\n",
      "input shape torch.Size([8, 3, 1024, 1024])\n",
      "after layer one torch.Size([8, 128, 512, 512])\n",
      "after layer two torch.Size([8, 256, 256, 256])\n",
      "after layer three torch.Size([8, 512, 128, 128])\n",
      "after layer four torch.Size([8, 1024, 64, 64])\n",
      "after layer 5 torch.Size([8, 1024, 32, 32])\n",
      "after layer 6 torch.Size([8, 1024, 16, 16])\n",
      "after layer 7 torch.Size([8, 1024, 8, 8])\n",
      "after layer 8 torch.Size([8, 1024, 4, 4])\n",
      "output shape, five torch.Size([8, 1, 1, 1]) end\n"
     ]
    }
   ],
   "source": [
    "# set to training mode\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "gen_loss_avg = []\n",
    "disc_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "\n",
    "# wandb.watch(generator, log_freq=100)\n",
    "# wandb.watch(discriminator, log_freq=100)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "best_model_gen = copy.deepcopy(generator.state_dict())\n",
    "best_model_disc = copy.deepcopy(discriminator.state_dict())\n",
    "best_epoch     = -1\n",
    "best_disc_loss = np.inf\n",
    "best_gen_loss = np.inf\n",
    "history = defaultdict(list)\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    \n",
    "    gen_loss_avg.append(0)\n",
    "    disc_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch in train_loader:\n",
    "        \n",
    "        # get dataset image and create real and fake labels for use in the loss\n",
    "        image_batch = image_batch.to(device)\n",
    "        label_real = torch.ones(image_batch.size(0), device=device)\n",
    "        label_fake = torch.zeros(image_batch.size(0), device=device)\n",
    "        \n",
    "        # generate a batch of images from samples of the latent prior\n",
    "        latent = torch.randn(image_batch.size(0), 100, 1, 1, device=device)\n",
    "        fake_image_batch = generator(latent)\n",
    "        # train discriminator to correctly classify real and fake\n",
    "        # (detach the computation graph of the generator and the discriminator,\n",
    "        # so that gradients are not backpropagated into the generator)\n",
    "        print(\"info real pred\")\n",
    "        real_pred = discriminator(image_batch).squeeze()\n",
    "        print(\"info fake pred\")\n",
    "        fake_pred = discriminator(fake_image_batch.detach()).squeeze()\n",
    "        disc_loss = 0.5 * (\n",
    "            criterion(real_pred, label_real) +\n",
    "            criterion(fake_pred, label_fake))\n",
    "        \n",
    "        disc_optimizer.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "        # train generator to output an image that is classified as real\n",
    "        fake_pred = discriminator(fake_image_batch).squeeze()\n",
    "        gen_loss = criterion(fake_pred, label_real)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        gen_loss_avg[-1] += gen_loss.item()\n",
    "        disc_loss_avg[-1] += disc_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        \n",
    "    gen_loss_avg[-1] /= num_batches\n",
    "    disc_loss_avg[-1] /= num_batches\n",
    "\n",
    "    visualize_generated_images(generator, latent, epoch)\n",
    "    \n",
    "    # # deep copy the model\n",
    "    # if disc_loss_avg[-1] >= best_disc_loss:\n",
    "    #     print(f\"{c_}Valid Score Improved ({best_disc_loss:0.4f} ---> {disc_loss_avg[-1]:0.4f})\")\n",
    "    #     best_disc_loss   = disc_loss_avg[-1]\n",
    "    #     best_epoch   = epoch\n",
    "    #     run.summary[\"Best Disriminator Loss\"]    = best_disc_loss\n",
    "    #     run.summary[\"Corresponding Generator Loss\"] = gen_loss_avg[-1] \n",
    "    #     run.summary[\"Best Epoch\"]   = best_epoch\n",
    "    #     best_model_gen = copy.deepcopy(generator.state_dict())\n",
    "    #     best_model_disc = copy.deepcopy(discriminator.state_dict())\n",
    "    #     # PATH = f\"best_epoch-{fold:02d}.bin\"\n",
    "    #     PATH = f\"{CURRENT_MODEL_DIR}/best_epoch.bin\"\n",
    "    #     torch.save(generator.state_dict(), PATH)\n",
    "    #     torch.save(discriminator.state_dict(), PATH)        \n",
    "    #     # Save a model file from the current directory\n",
    "    #     wandb.save(PATH)\n",
    "    #     print(f\"Model Saved{sr_}\")\n",
    "    \n",
    "    # # Log the metrics\n",
    "    # wandb.log({\"Discriminator Loss\": disc_loss_avg[-1], \n",
    "    #            \"Generator Loss\": gen_loss_avg[-1],})\n",
    "    \n",
    "    print('Epoch [%d / %d] average loss generator vs. discrim.: %f vs. %f' %\n",
    "          (epoch+1, CFG.epochs, gen_loss_avg[-1], disc_loss_avg[-1]))\n",
    "\n",
    "    \n",
    "history['Discriminator Loss'] = disc_loss_avg\n",
    "history['Generator Loss'] = gen_loss_avg\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "time_elapsed = end - start\n",
    "print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "print(\"Best Score: {:.4f}\".format(best_disc_loss))\n",
    "\n",
    "# run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7688.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "31490048/64/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
